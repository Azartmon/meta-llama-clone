{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "617f3e91-065a-435b-a405-91fbf2247a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_recipes.inference.prompt_format_utils import build_prompt, create_conversation, LLAMA_GUARD_CATEGORY\n",
    "from generation import Llama\n",
    "\n",
    "\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4513cd40-885a-4b3a-b675-4a09042b3431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hack for variables needed in Pytorch run:\n",
    "import os\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"MASTER_ADDR\"] = \"127.0.0.1\"\n",
    "os.environ[\"MASTER_PORT\"] = \"29500\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e187538-ce2a-4aa6-aa99-0f7b0768be11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Inference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "debb67cb-d6f8-4fc3-95b7-e5481a90bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pytorch_llm_eval(prompts: List[Tuple[List[str], AgentType, str, str, str]], ckpt_dir, logprobs: bool = False):\n",
    "    # defaults\n",
    "    temperature = 1\n",
    "    top_p = 1\n",
    "    max_seq_len = 4096\n",
    "    max_gen_len = 32\n",
    "    max_batch_size = 1\n",
    "\n",
    "    generator = Llama.build(\n",
    "            ckpt_dir=ckpt_dir,\n",
    "            tokenizer_path=ckpt_dir + \"/tokenizer.model\",\n",
    "            max_seq_len=max_seq_len,\n",
    "            max_batch_size=max_batch_size,\n",
    "        )\n",
    "\n",
    "\n",
    "    results: List[str] = []\n",
    "    for prompt in prompts:\n",
    "        formatted_prompt = build_prompt(\n",
    "                prompt[\"agent_type\"], \n",
    "                LLAMA_GUARD_CATEGORY, \n",
    "                create_conversation(prompt[\"prompt\"]))\n",
    "\n",
    "        # result = generator.single_prompt_completion(\n",
    "        #     formatted_prompt,\n",
    "        #     max_gen_len=max_gen_len,\n",
    "        #     temperature=temperature,\n",
    "        #     top_p=top_p,\n",
    "        # )\n",
    "        result = generator.text_completion(\n",
    "            [formatted_prompt],\n",
    "            temperature,\n",
    "            top_p,\n",
    "            max_gen_len,\n",
    "            logprobs\n",
    "        )\n",
    "        # getting the first value only, as only a single prompt was sent to the function\n",
    "        generation_result = result[0][\"generation\"]\n",
    "        prompt[\"result\"] = generation_result\n",
    "        if logprobs:\n",
    "            prompt[\"logprobs\"] = result[0][\"logprobs\"]\n",
    "\n",
    "        results.append(generation_result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b101a55-e163-4a58-b778-21d7b7cabdfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
