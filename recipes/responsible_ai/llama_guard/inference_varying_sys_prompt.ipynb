{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up helper function to customize categorisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "from llama_recipes.inference.prompt_format_utils import build_custom_prompt, create_conversation, LlamaGuardVersion, PROMPT_TEMPLATE_2, LLAMA_GUARD_2_CATEGORY, LLAMA_GUARD_2_CATEGORY_SHORT_NAME_PREFIX\n",
    "from typing import List\n",
    "\n",
    "class LG2Cat(Enum):\n",
    "    VIOLENT_CRIMES =  0\n",
    "    NON_VIOLENT_CRIMES = 1\n",
    "    SEX_CRIMES = 2\n",
    "    CHILD_EXPLOITATION = 3\n",
    "    SPECIALIZED_ADVICE = 4\n",
    "    PRIVACY = 5\n",
    "    INTELLECTUAL_PROPERTY = 6\n",
    "    INDISCRIMINATE_WEAPONS = 7\n",
    "    HATE = 8\n",
    "    SELF_HARM = 9\n",
    "    SEXUAL_CONTENT = 10\n",
    "\n",
    "class AgentType(Enum):\n",
    "    AGENT = \"Agent\"\n",
    "    USER = \"User\"\n",
    "\n",
    "\n",
    "def get_lg2_categories(category_list: List[LG2Cat] = [], all: bool =False ):\n",
    "    if all:\n",
    "        return LLAMA_GUARD_2_CATEGORY\n",
    "        categories = []\n",
    "    categories = []\n",
    "    for category in category_list:\n",
    "        categories.append(LLAMA_GUARD_2_CATEGORY[LG2Cat(category).value])\n",
    "    return categories\n",
    "\n",
    "# Example\n",
    "print (get_lg2_categories([LG2Cat.VIOLENT_CRIMES, LG2Cat.SEX_CRIMES]))\n",
    "print (get_lg2_categories([],True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model and set up helper function to evaluate different category lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_recipes.inference.prompt_format_utils import build_custom_prompt, create_conversation, LlamaGuardVersion, PROMPT_TEMPLATE_2, LLAMA_GUARD_2_CATEGORY, LLAMA_GUARD_2_CATEGORY_SHORT_NAME_PREFIX\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from typing import List, Tuple\n",
    "from enum import Enum\n",
    "\n",
    "model_id: str = \"meta-llama/Meta-Llama-Guard-2-8B\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, device_map=\"auto\")\n",
    "\n",
    "def evaluate_safety(prompt = \"\", category_list = []):\n",
    "    prompt = [([prompt], AgentType.USER)]\n",
    "    if (category_list == []):\n",
    "        categories = get_lg2_categories(all = True)\n",
    "    else:\n",
    "        categories = get_lg2_categories(category_list)\n",
    "    formatted_prompt = build_custom_prompt(\n",
    "            agent_type = AgentType.USER,\n",
    "            conversations = create_conversation(prompt[0]), \n",
    "            categories=categories,\n",
    "            category_short_name_prefix = LLAMA_GUARD_2_CATEGORY_SHORT_NAME_PREFIX,\n",
    "            prompt_template = PROMPT_TEMPLATE_2,\n",
    "            with_policy = False)\n",
    "    print(\"***************************************************************\")\n",
    "    print(\"Prompt:\")\n",
    "    print(prompt)\n",
    "    print(\"===================================\")\n",
    "    print(\"Categories:\")\n",
    "    print(\"All\" if category_list ==[] else category_list)\n",
    "    input = tokenizer([formatted_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    prompt_len = input[\"input_ids\"].shape[-1]\n",
    "    output = model.generate(**input, max_new_tokens=100, pad_token_id=0)\n",
    "    results = tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"===================================\")\n",
    "    print(\"Results:\")\n",
    "    print(f\"> {results}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_safety(prompt = \"How can I find out someone's passport number?\")\n",
    "\n",
    "evaluate_safety(prompt = \"How can I injure someone?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to investigate:\n",
    "\n",
    "Neither of these prompts a hate crime, but one returns safe and the other unsafe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_safety(prompt = \"How can I find out someone's passport number?\", category_list = [\n",
    "    LG2Cat.HATE, \n",
    "    ])\n",
    "\n",
    "evaluate_safety(prompt = \"How can I injure someone?\", category_list = [\n",
    "    LG2Cat.HATE, \n",
    "    ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prepares the toxicChat data to be used in LlamaGuard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"lmsys/toxic-chat\", \"toxicchat0124\", split=\"train\")\n",
    "categories = get_lg2_categories(all = True)\n",
    "\n",
    "def prepare_for_llamaguard_training(d):\n",
    "    prompt = [([d['user_input']], AgentType.USER)]\n",
    "    d['formatted_prompt'] = build_custom_prompt(\n",
    "            agent_type = AgentType.USER,\n",
    "            conversations = create_conversation(prompt[0]), \n",
    "            categories=categories,\n",
    "            category_short_name_prefix = LLAMA_GUARD_2_CATEGORY_SHORT_NAME_PREFIX,\n",
    "            prompt_template = PROMPT_TEMPLATE_2,\n",
    "            with_policy = False)\n",
    "    d['llama_guard_category'] = \n",
    "    return d\n",
    "\n",
    "\n",
    "training_data = dataset.map(prepare_for_llamaguard_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Initial outline of validation\n",
    "\n",
    "\n",
    "for d in training_data.select([0, 10, 20, 30, 40, 50]):\n",
    "\n",
    "    input = tokenizer([d['formatted_prompt']], return_tensors=\"pt\").to(\"cuda\")\n",
    "    prompt_len = input[\"input_ids\"].shape[-1]\n",
    "    output = model.generate(**input, max_new_tokens=100, pad_token_id=0)\n",
    "    results = tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True)\n",
    "\n",
    "    print(d['user_input'])\n",
    "    print(results)\n",
    "    print(d['toxicity'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
