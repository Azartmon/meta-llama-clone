{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33965e0c-2e73-4e7c-ac8a-0e4c7419d440",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from logging import getLogger\n",
    "from typing import (\n",
    "    Any,\n",
    "    List,\n",
    "    Iterator,\n",
    "    Dict,\n",
    "    Optional,\n",
    "    Union,\n",
    "    Literal,\n",
    "    AbstractSet,\n",
    "    Collection\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdab350b-a816-46b2-994c-a1de4171f130",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import tiktoken\n",
    "    from tiktoken.load import load_tiktoken_bpe\n",
    "\n",
    "    \n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Please install tiktoken, blobfile and, lxml with `pip install tiktoken blobfile lxml`.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9e05021-6430-4100-a805-6823d25836a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = getLogger()\n",
    "\n",
    "class BaseTokenizer(ABC):\n",
    "    def __init__(self, model_path: str) -> None:\n",
    "        assert os.path.exists(\n",
    "            model_path\n",
    "        ), f\"The tokenizer path does not exist: {model_path}\"\n",
    "        self._bos_id = 0\n",
    "        self._eos_id = 1\n",
    "        self._pad_id = -1\n",
    "        self._n_words = 2\n",
    "\n",
    "    @property\n",
    "    def bos_id(self) -> int:\n",
    "        return self._bos_id\n",
    "\n",
    "    @property\n",
    "    def eos_id(self) -> int:\n",
    "        return self._eos_id\n",
    "\n",
    "    @property\n",
    "    def pad_id(self) -> int:\n",
    "        return self._pad_id\n",
    "\n",
    "    @property\n",
    "    def n_words(self) -> int:\n",
    "        return self._n_words\n",
    "\n",
    "    @abstractmethod\n",
    "    def encode(self, *args: Any, **kwargs: Any) -> List[int]: ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def decode(self, *args: Any, **kwargs: Any) -> str: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d90e9b9-659a-46cc-9ae7-80ded9314bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyre-strict\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class TokenizerArgs():\n",
    "    \"\"\"\n",
    "    A data class that holds the arguments for a tokenizer. Default value is set from llama3 model\n",
    "    Attributes:\n",
    "        model (str): The name of the model to be used for tokenization. Default is \"cl_toplang_128k\".\n",
    "        directory (str): The directory where the model is located. Default is an empty string.(not exposed yet)\n",
    "        tokenizer_cls (str): The class name of the tokenizer. Default is \"TiktokenTokenizer\".\n",
    "        num_reserved_special_tokens (int): The number of special tokens reserved. Default is 256.\n",
    "    Property:\n",
    "        path (str): The full path to the model, combining the directory and model name.\n",
    "    \"\"\"\n",
    "\n",
    "    model: str = \"cl_toplang_128k\"\n",
    "    directory: str = \"\"\n",
    "    tokenizer_cls: str = \"TiktokenTokenizer\"\n",
    "    num_reserved_special_tokens: int = 256\n",
    "\n",
    "    @property\n",
    "    def path(self) -> str:\n",
    "        return os.path.join(self.directory, self.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f447b5-afd4-446f-9813-406daa017f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_whitespaces_or_nonwhitespaces(\n",
    "    s: str, max_consecutive_slice_len: int\n",
    ") -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Split the string `s` so that each substring contains no more than `max_consecutive_slice_len`\n",
    "    consecutive whitespaces or consecutive non-whitespaces\n",
    "    \"\"\"\n",
    "    current_slice_len = 0\n",
    "    current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
    "    slice_start = 0\n",
    "\n",
    "    for i in range(len(s)):\n",
    "        is_now_space = s[i].isspace()\n",
    "\n",
    "        if current_slice_is_space ^ is_now_space:\n",
    "            current_slice_len = 1\n",
    "            current_slice_is_space = is_now_space\n",
    "        else:\n",
    "            current_slice_len += 1\n",
    "            if current_slice_len > max_consecutive_slice_len:\n",
    "                yield s[slice_start:i]\n",
    "                slice_start = i\n",
    "                current_slice_len = 1\n",
    "    yield s[slice_start:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a6a6152-f833-4b3e-b636-46af2baa102f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TiktokenTokenizer(BaseTokenizer):\n",
    "    BASIC_SPECIAL_TOKENS = [\n",
    "        \"<|begin_of_text|>\",\n",
    "        \"<|end_of_text|>\",\n",
    "        \"<|fim_prefix|>\",\n",
    "        \"<|fim_middle|>\",\n",
    "        \"<|fim_suffix|>\",\n",
    "    ]\n",
    "    CL100K_PATTERN = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "    def __init__(self, args: TokenizerArgs) -> None:\n",
    "        super().__init__(args.path)\n",
    "\n",
    "        mergeable_ranks = load_tiktoken_bpe(args.path)\n",
    "        all_special_tokens_with_ids = self._get_all_special_tokens_with_ids(\n",
    "            args, len(mergeable_ranks)\n",
    "        )\n",
    "\n",
    "        self._tok_model = tiktoken.Encoding(\n",
    "            name=args.model,\n",
    "            pat_str=TiktokenTokenizer.CL100K_PATTERN,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens={**all_special_tokens_with_ids},\n",
    "        )\n",
    "        logger.info(f\"Reloaded Tiktoken model from {args.path}\")\n",
    "\n",
    "        self._bos_id: int = self.encode(\n",
    "            TiktokenTokenizer.BASIC_SPECIAL_TOKENS[0],\n",
    "            bos=False,\n",
    "            eos=False,\n",
    "            allowed_special=\"all\",\n",
    "        )[0]\n",
    "        self._eos_id: int = self.encode(\n",
    "            TiktokenTokenizer.BASIC_SPECIAL_TOKENS[1],\n",
    "            bos=False,\n",
    "            eos=False,\n",
    "            allowed_special=\"all\",\n",
    "        )[0]\n",
    "        self._pad_id = -1\n",
    "        self._n_words: int = self._tok_model.n_vocab\n",
    "\n",
    "        logger.info(\n",
    "            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n",
    "        )\n",
    "\n",
    "    def _get_all_special_tokens_with_ids(\n",
    "        self, args: TokenizerArgs, num_base_tokens: int\n",
    "    ) -> Dict[str, int]:\n",
    "\n",
    "        all_special_tokens = TiktokenTokenizer.BASIC_SPECIAL_TOKENS\n",
    "\n",
    "        assert len(set(all_special_tokens)) == len(\n",
    "            all_special_tokens\n",
    "        ), \"Special tokens must be unique.\"\n",
    "\n",
    "        n_vocab = num_base_tokens + args.num_reserved_special_tokens\n",
    "        assert (\n",
    "            n_vocab % 8 == 0\n",
    "        ), \"Vocabulary size must be divisible by 8 for vocabulary parallelism on 8 GPUs\"\n",
    "\n",
    "        assert (\n",
    "            len(all_special_tokens) <= args.num_reserved_special_tokens\n",
    "        ), \"The total number of basic and extra special tokens exceeds the number of reserved tokens.\"\n",
    "\n",
    "        reserved_tokens = [\n",
    "            f\"<|reserved_special_token_{i}|>\"\n",
    "            for i in range(args.num_reserved_special_tokens - len(all_special_tokens))\n",
    "        ]\n",
    "        all_special_tokens = (\n",
    "            all_special_tokens[:-1] + reserved_tokens + [all_special_tokens[-1]]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            token: num_base_tokens + i for i, token in enumerate(all_special_tokens)\n",
    "        }\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        s: str,\n",
    "        bos: bool,\n",
    "        eos: bool,\n",
    "        allowed_special: Optional[Union[Literal[\"all\"], AbstractSet[str]]] = None,\n",
    "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> List[int]:\n",
    "        if allowed_special is None:\n",
    "            allowed_special = set()\n",
    "        assert type(s) is str\n",
    "\n",
    "        # The tiktoken tokenizer can handle <=400k chars without\n",
    "        # pyo3_runtime.PanicException (may go beyond 400k)\n",
    "        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
    "\n",
    "        # Tiktoken is very bad at handling long sequences where either no whitespaces or only whitespaces:\n",
    "        # https://github.com/openai/tiktoken/issues/195\n",
    "        # Here we iterate over subsequences and split if we exceed the limit\n",
    "        # of max consequtive non-whitespace or whitespace characters.\n",
    "        MAX_NO_WHITESPACES_CHARS = 25_000\n",
    "\n",
    "        # TODO check if MAX_NO_WHITESPACES_CHARS already fixes the issue with TIKTOKEN_MAX_ENCODE_CHARS\n",
    "\n",
    "        substrs: List[str] = []\n",
    "        t = []\n",
    "        for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS):\n",
    "            substr = s[i : i + TIKTOKEN_MAX_ENCODE_CHARS]\n",
    "            sliced_substr = split_whitespaces_or_nonwhitespaces(\n",
    "                substr, MAX_NO_WHITESPACES_CHARS\n",
    "            )\n",
    "            substrs.extend(sliced_substr)\n",
    "        for substr in substrs:\n",
    "            # By default, setting disallowed_special=() encodes a string by\n",
    "            # ignoring special tokens. Specifically:\n",
    "            # - Setting `disallowed_special` to () will cause all text\n",
    "            #   corresponding to special tokens to be encoded as natural\n",
    "            #   text (insteading of raising an error).\n",
    "            # - Setting `allowed_special` to \"all\" will treat all text\n",
    "            #   corresponding to special tokens to be encoded as special tokens\n",
    "            t.extend(\n",
    "                self._tok_model.encode(\n",
    "                    substr,\n",
    "                    allowed_special=allowed_special,\n",
    "                    disallowed_special=disallowed_special,\n",
    "                )\n",
    "            )\n",
    "        if bos:\n",
    "            t.insert(0, self.bos_id)\n",
    "        if eos:\n",
    "            t.append(self.eos_id)\n",
    "        return t\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        tokens: List[int],\n",
    "        cut_at_eos: bool = True,\n",
    "        *args: Any,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        if cut_at_eos:\n",
    "            for k, t in enumerate(tokens):\n",
    "                if t == self.eos_id:\n",
    "                    tokens = tokens[: k + 1]\n",
    "                    break\n",
    "        tokens = [token for token in tokens if token not in [self.bos_id, self.eos_id]]\n",
    "        return self._tok_model.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aed8a311-7676-47f3-879a-a3d679734ad6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[128000, 9906, 4435, 0, 128001]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__' and '__file__' not in globals():\n",
    "    args = TokenizerArgs()\n",
    "    args.directory = \"/home/ubuntu/projects/llama/models/llama_guard-v2/\"\n",
    "    \n",
    "    tokenizer = TiktokenTokenizer(args)\n",
    "    \n",
    "    print(tokenizer.encode(\"Hello World!\", True, True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "29dffafc-bce9-4c0f-96db-8c245c7f356a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "#WIP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SimpleTiktokenTokenizer:\n",
    "    \"\"\"tokenizing and encoding/decoding text using tiktoken.\"\"\"\n",
    "\n",
    "    BASIC_SPECIAL_TOKENS = [\n",
    "        \"<|begin_of_text|>\",\n",
    "        \"<|end_of_text|>\",\n",
    "        \"<|fim_prefix|>\",\n",
    "        \"<|fim_middle|>\",\n",
    "        \"<|fim_suffix|>\",\n",
    "    ]\n",
    "    CL100K_PATTERN = r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"  # noqa: E501\n",
    "\n",
    "    def __init__(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the Tokenizer with a SentencePiece model.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): The path to the SentencePiece model file.\n",
    "        \"\"\"\n",
    "        # reload tokenizer\n",
    "        assert os.path.isfile(model_path), model_path\n",
    "        self.model_path = model_path\n",
    "        mergeable_ranks = load_tiktoken_bpe(self.model_path)\n",
    "        \n",
    "        \n",
    "        self.sp_model = tiktoken.Encoding(\n",
    "            name=\"custom_cl128k\",\n",
    "            pat_str=self.CL100K_PATTERN,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens={**all_special_tokens_with_ids},\n",
    "        )\n",
    "        logger.info(f\"Reloaded tiktoken model from {model_path}\")\n",
    "\n",
    "        # BOS / EOS token IDs\n",
    "        self.n_words: int = self.sp_model.vocab_size()\n",
    "        self.bos_id: int = self.sp_model.bos_id()\n",
    "        self.eos_id: int = self.sp_model.eos_id()\n",
    "        self.pad_id: int = self.sp_model.pad_id()\n",
    "        logger.info(\n",
    "            f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\"\n",
    "        )\n",
    "        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n",
    "\n",
    "    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n",
    "        \"\"\"\n",
    "        Encodes a string into a list of token IDs.\n",
    "\n",
    "        Args:\n",
    "            s (str): The input string to be encoded.\n",
    "            bos (bool): Whether to prepend the beginning-of-sequence token.\n",
    "            eos (bool): Whether to append the end-of-sequence token.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: A list of token IDs.\n",
    "        \"\"\"\n",
    "        assert type(s) is str\n",
    "        t = self.sp_model.encode(s)\n",
    "        if bos:\n",
    "            t = [self.bos_id] + t\n",
    "        if eos:\n",
    "            t = t + [self.eos_id]\n",
    "        return t\n",
    "\n",
    "    def decode(self, t: List[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decodes a list of token IDs into a string.\n",
    "\n",
    "        Args:\n",
    "            t (List[int]): The list of token IDs to be decoded.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        return self.sp_model.decode(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e67699-7eb6-471c-bc9e-7af90e51de1d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
