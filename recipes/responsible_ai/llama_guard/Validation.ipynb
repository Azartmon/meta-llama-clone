{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c29b97c-1f79-4f26-a019-a1f85a85b9c6",
   "metadata": {},
   "source": [
    "# [WIP] Validating the output of Llama Guard quantized and unquantized\n",
    "\n",
    "This notebook aims to show how to validate Llama Guard performance on a given dataset. The script loads a given dataset and evaluates each prompt individually against Llama Guard. To evaluate performance, we calculate the averate precision of the binary classification for a given prompt. Llama Guard can be run usgin Meta provided weights or directly from Hugging Face. \n",
    "\n",
    "## Dataset format\n",
    "The dataset should be in a `jsonl` file, with an object per line, following this structure:\n",
    "```\n",
    "{\n",
    "    \"prompt\": \"user_input\",\n",
    "    \"generation\": \"model_response\",\n",
    "    \"label\": \"good/bad\", \n",
    "    \"unsafe_content\": [\"O1\"]\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "The `label` has a `good` or `bad` value to define if the content is considered safe or unsafe, respectively.\n",
    "\n",
    "The `unsafe_content` field contains a list of the categories the prompt is violating.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a33bcb2-1c42-4abb-8e4f-fe5df88fca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Tuple\n",
    "\n",
    "from enum import Enum\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d266b2-c232-4c7e-8beb-624a4719d9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (0.21.4)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from huggingface_hub) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from huggingface_hub) (4.62.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from huggingface_hub) (4.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from requests->huggingface_hub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from requests->huggingface_hub) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/llama-recipes/lib/python3.10/site-packages (from requests->huggingface_hub) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "%run Inference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f361b27c-8601-48e6-b453-14ed051379c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Type(Enum):\n",
    "    HF = \"HF\"\n",
    "    PYTORCH = \"PYTORCH\"\n",
    "\n",
    "def format_prompt(entry, agent_type: AgentType):\n",
    "    prompts = []\n",
    "    if agent_type == AgentType.USER:\n",
    "        prompts = [entry[\"prompt\"]]\n",
    "    else:\n",
    "        prompts = [entry[\"prompt\"], entry[\"generation\"]]\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompts,\n",
    "        \"agent_type\": agent_type,\n",
    "        \"label\": entry[\"label\"],\n",
    "        \"unsafe_content\": entry[\"unsafe_content\"],\n",
    "        \"idx\": entry[\"idx\"]\n",
    "    }\n",
    "\n",
    "def validate_agent_type(value):\n",
    "    try:\n",
    "        return AgentType(value)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid AgentType. Choose from: {[agent_type.value for agent_type in AgentType]}\")\n",
    "\n",
    "\n",
    "\n",
    "def run_validation(jsonl_file_path, agent_type, type: Type, load_in_8bit: bool = True, ckpt_dir = None):\n",
    "\n",
    "    input_file_path = Path(jsonl_file_path)\n",
    "\n",
    "    agent_type = validate_agent_type(agent_type)\n",
    "    \n",
    "    # Preparing prompts\n",
    "    prompts: List[Tuple[List[str], AgentType, str, str, str]] = []\n",
    "    with open(jsonl_file_path, \"r\") as f:\n",
    "        # temp\n",
    "        index = 0 \n",
    "        for i, line in enumerate(f):\n",
    "            if index == 200:\n",
    "                break\n",
    "            index += 1\n",
    "\n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            # Format prompt and add to list\n",
    "            prompt = format_prompt(entry, agent_type)\n",
    "            prompts.append(prompt)\n",
    "            \n",
    "            \n",
    "\n",
    "    # Executing evaluation\n",
    "    start = time.time()\n",
    "    if type is Type.HF:\n",
    "        llm_eval(prompts, load_in_8bit, True)\n",
    "    else:\n",
    "        pytorch_llm_eval(prompts, ckpt_dir, True)\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f\"evaluation executed in {end - start} seconds\")\n",
    "        \n",
    "    average_precision = parse_logprobs(prompts, type)\n",
    "    print(f\"average precision {average_precision:.2%}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a9562e5-6580-491c-a418-0d8fe9771cf7",
   "metadata": {},
   "source": [
    "## Average presicion\n",
    "\n",
    "This function calculates the average precision score for a set of prompts based on their log probabilities and labels. \n",
    "\n",
    "The `prompts` contain the logprobs calculated for each result by Llama Guard when evaluating the prompts or prompt and generation. \n",
    "\n",
    "The `type` is used to identify if the logprobs are comming from a Hugging Face model or plain pytorch model.\n",
    "\n",
    "The logprob is converted back into probability by exponentiating it (`np.exp`)\n",
    "\n",
    "The probability for `unsafe` when the result is `safe` is calculated using the heuristic 1 - `safe`. As this is a banary classification problem, it should be close to the real value for `unsafe`.\n",
    "\n",
    "The average presicion is calculated with the binary labels from the expected value for each prompt or prompt/generation pair and the probability of generating the unsafe token for each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d04d0097-951f-477f-8493-48008cc5c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_logprobs(prompts, type: Type):\n",
    "    positive_class_probs = []\n",
    "    for prompt in prompts:\n",
    "        prob = np.exp(prompt[\"logprobs\"][0]) if type is Type.PYTORCH else np.exp(prompt[\"logprobs\"][0][1])\n",
    "        if \"unsafe\" in prompt[\"result\"]:\n",
    "            positive_class_probs.append(prob)\n",
    "        else:\n",
    "            # Using heuristic 1 - `safe` probability to calculate the probability of a non selected token in a binary classification\n",
    "            positive_class_probs.append(1 - prob)\n",
    "        \n",
    "    binary_labels = [1 if prompt[\"label\"] == \"bad\" else 0 for prompt in prompts]\n",
    "\n",
    "    return average_precision_score(binary_labels, positive_class_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57483ddd-f3f0-419f-89e5-53fe6d584c8a",
   "metadata": {},
   "source": [
    "**Note:** If you get a `Address already in use` error when running with a local llama guard model, change the port by setting the environment variable to a new one. e.g.: `os.environ[\"MASTER_PORT\"] = \"29501\"` For more details, check `Inference.ipynb`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b660e42c-ac80-4538-bb55-d0fb097949e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> initializing model parallel with size 1\n",
      "> initializing ddp with size 1\n",
      "> initializing pipeline with size 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama-recipes/lib/python3.10/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded in 7.48 seconds\n",
      "evaluation executed in 100.73966026306152 seconds\n",
      "average precision 95.58%\n"
     ]
    }
   ],
   "source": [
    "prompts_file = \"valid_prompts_6cat_1122.jsonl\"\n",
    "responses_file = \"valid_responses_6cat_1122.jsonl\"\n",
    "\n",
    "run_validation(prompts_file, AgentType.USER, Type.PYTORCH, ckpt_dir = \"../../../../llama/models/guard-llama/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e31c6df-d0c2-417f-be48-a5532c79b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up the cache from running the previous validation\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41c1c1f-ca75-4353-8292-852acc73153e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e05ee76eae04322b4cc05abe88d99ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Login to HF to access the model\n",
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef07df70-f6bc-459a-9b3d-4cb3bf698757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003801584243774414,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 26,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fbbd6f29f5412bbf22b2bee3b726d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama-recipes/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation executed in 156.22336435317993 seconds\n",
      "average precision 95.33%\n"
     ]
    }
   ],
   "source": [
    "run_validation(prompts_file, AgentType.USER, Type.HF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd40ec-57dc-4ac8-aa38-3d274c4e0b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
