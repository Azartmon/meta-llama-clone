{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c29b97c-1f79-4f26-a019-a1f85a85b9c6",
   "metadata": {},
   "source": [
    "# WIP Validating the output of Llama Guard quantized and unquantized\n",
    "\n",
    "This notebook aims to show how to validate Llama Guard performance on a given dataset. The dataset format is:\n",
    "```\n",
    "{\n",
    "    \"prompt\": \"user_input\",\n",
    "    \"generation\": \"model_response\",\n",
    "    \"label\": \"good/bad\", \n",
    "    \"unsafe_content\": [\"O1\"]\n",
    "}\n",
    "```\n",
    "The `label` has a `good` or `bad` value to define if the content is considered safe or unsafe, respectively.\n",
    "\n",
    "The scripts takes the prompts from the file and runs them through Llama Guard (loaded from Hugging face) and reports the resulting logprobs.\n",
    "\n",
    "Note: the logprobs for the inputs considered to be safe are calculated using a simple heuristic: \n",
    " 1 - `safe` probability to avoid getting the probability from the model for a non selected token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a33bcb2-1c42-4abb-8e4f-fe5df88fca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# from examples.llama_guard.inference import llm_eval, standard_llm_eval, AgentType\n",
    "from typing import List, Tuple\n",
    "\n",
    "from enum import Enum\n",
    "import time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16d266b2-c232-4c7e-8beb-624a4719d9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Inference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9233cbf6-dac2-436f-b812-ee7aea0e8743",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Pytorch_Inference.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f361b27c-8601-48e6-b453-14ed051379c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Type(Enum):\n",
    "    HF = \"HF\"\n",
    "    PYTORCH = \"PYTORCH\"\n",
    "\n",
    "def format_prompt(entry, agent_type: AgentType):\n",
    "    prompts = []\n",
    "    if agent_type == AgentType.USER:\n",
    "        prompts = [entry[\"prompt\"]]\n",
    "    else:\n",
    "        prompts = [entry[\"prompt\"], entry[\"generation\"]]\n",
    "\n",
    "    return {\n",
    "        \"prompt\": prompts,\n",
    "        \"agent_type\": agent_type,\n",
    "        \"label\": entry[\"label\"],\n",
    "        \"unsafe_content\": entry[\"unsafe_content\"],\n",
    "        \"idx\": entry[\"idx\"]\n",
    "    }\n",
    "\n",
    "def validate_agent_type(value):\n",
    "    try:\n",
    "        return AgentType(value)\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Invalid AgentType. Choose from: {[agent_type.value for agent_type in AgentType]}\")\n",
    "\n",
    "def parse_logprobs(prompts):\n",
    "    positive_class_probs = []\n",
    "    for prompt in prompts:\n",
    "        prob = np.exp(prompt[\"logprobs\"][0]) if isinstance(prompt[\"logprobs\"][0], float) else np.exp(prompt[\"logprobs\"][0][1])\n",
    "        if \"unsafe\" in prompt[\"result\"]:\n",
    "            positive_class_probs.append(prob)\n",
    "        else:\n",
    "            # Using heuristic 1 - `safe` probability to avoid getting the probability from the model for a non selected token.\n",
    "            positive_class_probs.append(1 - prob)\n",
    "        \n",
    "    binary_labels = [1 if prompt[\"label\"] == \"bad\" else 0 for prompt in prompts]\n",
    "\n",
    "    return average_precision_score(binary_labels, positive_class_probs)\n",
    "\n",
    "def run_validation(jsonl_file_path, agent_type, type: Type, load_in_8bit: bool = True, logprobs: bool = True):\n",
    "\n",
    "    input_file_path = Path(jsonl_file_path)\n",
    "\n",
    "    # Extracting filename without extension\n",
    "    filename = input_file_path.stem\n",
    "    directory = input_file_path.parent\n",
    "\n",
    "    agent_type = validate_agent_type(agent_type)\n",
    "    \n",
    "    # Preparing prompts\n",
    "    prompts: List[Tuple[List[str], AgentType, str, str, str]] = []\n",
    "    with open(jsonl_file_path, \"r\") as f:\n",
    "        # temp\n",
    "        index = 0 \n",
    "        for i, line in enumerate(f):\n",
    "            if index == 200:\n",
    "                break\n",
    "            index += 1\n",
    "\n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            # Call Llama Guard and get its output\n",
    "            prompt = format_prompt(entry, agent_type)\n",
    "            prompts.append(prompt)\n",
    "            \n",
    "            \n",
    "\n",
    "    # Executing evaluation\n",
    "    start = time.time()\n",
    "    \n",
    "    \n",
    "    if type is Type.HF:\n",
    "        llm_eval(prompts, load_in_8bit, logprobs)\n",
    "    else:\n",
    "        pytorch_llm_eval(prompts, \"../../../llama/models/guard-llama/\", logprobs)\n",
    "    end = time.time()\n",
    "    print(f\"evaluation executed in {end - start} seconds\")\n",
    "    \n",
    "    parsed_results = {}\n",
    "    if logprobs:\n",
    "        average_precision = parse_logprobs(prompts)\n",
    "        parsed_results[\"average_precision\"] = average_precision\n",
    "        print(f\"average precision {average_precision:.2%}\")\n",
    "\n",
    "    # Output filenames and paths\n",
    "    # current_date = datetime.now().strftime('%Y-%m-%d-%H:%M')\n",
    "    # output_filename = f\"{filename}_{agent_type.value}_{type.value}_{'load_in_8bit' if load_in_8bit else 'noquant'}_output_{current_date}.jsonl\"\n",
    "    # output_stats_filename = f\"{filename}_{agent_type.value}_{type.value}_{'load_in_8bit' if load_in_8bit else 'noquant'}_stats_{current_date}.json\"\n",
    "    # output_file_path = directory / output_filename\n",
    "    # output_stats_file_path = directory / output_stats_filename\n",
    "\n",
    "    # # Write the list of dictionaries to the file\n",
    "    # with open(output_file_path, 'w') as file:\n",
    "    #     for prompt in prompts:\n",
    "    #         # Serialize each dictionary to a JSON formatted string\n",
    "    #         prompt.pop(\"logprobs\", None)\n",
    "    #         json_str = json.dumps(prompt, default=lambda o: o.value if isinstance(o, Enum) else o)\n",
    "    #         # Write the JSON string to the file followed by a newline\n",
    "    #         file.write(json_str + '\\n')\n",
    "        \n",
    "    # # Write the stats from the run\n",
    "    # with open(output_stats_file_path, 'w') as f:\n",
    "    #     json.dump(parsed_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b660e42c-ac80-4538-bb55-d0fb097949e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005715370178222656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 26,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8421e61a325946118d1b2c363fdf3776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/llama-recipes/lib/python3.10/site-packages/transformers/generation/utils.py:1518: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation executed in 155.58132243156433 seconds\n",
      "average precision 95.25%\n"
     ]
    }
   ],
   "source": [
    "prompts_file = \"valid_prompts_6cat_1122.jsonl\"\n",
    "responses_file = \"valid_responses_6cat_1122.jsonl\"\n",
    "\n",
    "run_validation(prompts_file, AgentType.USER, Type.HF)\n",
    "# run_validation(prompts_file, AgentType.USER, Type.PYTORCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef07df70-f6bc-459a-9b3d-4cb3bf698757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
