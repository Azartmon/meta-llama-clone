{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt Formatting Sample\n",
        "\n",
        "This notbook is intended to showcase how to correctly format a Llama 3 prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import (\n",
        "    List,\n",
        "    Literal,\n",
        "    Sequence,\n",
        "    TypedDict,\n",
        ")\n",
        "\n",
        "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
        "\n",
        "class Message(TypedDict):\n",
        "    role: Role\n",
        "    content: str\n",
        "\n",
        "\n",
        "Dialog = Sequence[Message]\n",
        "\n",
        "class ChatFormat:\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_header( message: Message) -> List[str]:\n",
        "        prompts = []\n",
        "        prompts.append(\"<|start_header_id|>\")\n",
        "        prompts.extend(message[\"role\"])\n",
        "        prompts.append(\"<|end_header_id|>\")\n",
        "        prompts.extend(\"\\n\\n\")\n",
        "        return prompts\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_message(message: Message) -> List[str]:\n",
        "        prompts = ChatFormat.encode_header(message)\n",
        "        prompts.extend(message[\"content\"].strip())\n",
        "        prompts.append(\"<|eot_id|>\")\n",
        "        return prompts\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_dialog_prompt(dialog: Dialog) -> str:\n",
        "        prompts = [\"<|begin_of_text|>\"]\n",
        "        for message in dialog:\n",
        "            prompts.extend(ChatFormat.encode_message(message))\n",
        "        # Add the start of an assistant message for the model to complete.\n",
        "        prompts.extend(ChatFormat.encode_header({\"role\": \"assistant\", \"content\": \"\"}))\n",
        "        return \"\".join(prompts)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def format_python_prompt_output(system_prompt, user_prompt_1, assistant_response, user_prompt_2) -> str:\n",
        "        template = f\"\"\"from typing import (\n",
        "    List,\n",
        "    Literal,\n",
        "    Sequence,\n",
        "    TypedDict,\n",
        ")\n",
        "\n",
        "Role = Literal[\"system\", \"user\", \"assistant\"]\n",
        "\n",
        "class Message(TypedDict):\n",
        "    role: Role\n",
        "    content: str\n",
        "\n",
        "\n",
        "Dialog = Sequence[Message]\n",
        "\n",
        "class ChatFormat:\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_header( message: Message) -> List[str]:\n",
        "        prompts = []\n",
        "        prompts.append(\"<|start_header_id|>\")\n",
        "        prompts.extend(message[\"role\"])\n",
        "        prompts.append(\"<|end_header_id|>\")\n",
        "        prompts.extend(\"\\\\n\\\\n\")\n",
        "        return prompts\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_message(message: Message) -> List[str]:\n",
        "        prompts = ChatFormat.encode_header(message)\n",
        "        prompts.extend(message[\"content\"].strip())\n",
        "        prompts.append(\"<|eot_id|>\")\n",
        "        return prompts\n",
        "\n",
        "    @staticmethod\n",
        "    def encode_dialog_prompt(dialog: Dialog) -> str:\n",
        "        prompts = [\"<|begin_of_text|>\"]\n",
        "        for message in dialog:\n",
        "            prompts.extend(ChatFormat.encode_message(message))\n",
        "        # Add the start of an assistant message for the model to complete.\n",
        "        prompts.extend(ChatFormat.encode_header({{\"role\": \"assistant\", \"content\": \"\"}}))\n",
        "        return \"\".join(prompts)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    dialog: Dialog = []\"\"\"\n",
        "        if system_prompt:\n",
        "            system_section = f\"\"\"\n",
        "    dialog.append({{   \"role\": \"system\", \"content\": \"{system_prompt}\", }})\"\"\"\n",
        "            template += system_section\n",
        "        user_section = f\"\"\"\n",
        "    dialog.append({{   \"role\": \"user\", \"content\": \"{user_prompt_1}\", }})\"\"\"\n",
        "        template += user_section\n",
        "        if assistant_response:\n",
        "            assistant_section = f\"\"\"\n",
        "    dialog.append({{   \"role\": \"assistant\", \"content\": \"{assistant_response}\", }})\"\"\"\n",
        "            template += assistant_section\n",
        "        if user_prompt_2:\n",
        "            second_user_prompt_section = f\"\"\"\n",
        "    dialog.append({{   \"role\": \"user\", \"content\": \"{user_prompt_2}\", }})\"\"\"\n",
        "            template += second_user_prompt_section\n",
        "\n",
        "        template += f\"\"\"\n",
        "    print(ChatFormat.encode_dialog_prompt(dialog))\"\"\"\n",
        "\n",
        "        return template\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "from prompt_utils import ChatFormat, Dialog\n",
        "\n",
        "SINGLE_TURN = \"Single Turn\"\n",
        "MULTI_TURN = \"Multi Turn\"\n",
        "\n",
        "def prompt_template_dropdown_listener(value):\n",
        "    # This function will be called whenever the value of the dropdown changes\n",
        "    if value == SINGLE_TURN:\n",
        "        return {\n",
        "            assistant_response: gr.Textbox(elem_id=\"assistant_response\", visible = False),\n",
        "            user_prompt_2: gr.Textbox(elem_id=\"user_prompt_2\", visible = False)\n",
        "        }\n",
        "    else:\n",
        "        return {\n",
        "            assistant_response: gr.Textbox(elem_id=\"assistant_response\", visible = True),\n",
        "            user_prompt_2: gr.Textbox(elem_id=\"user_prompt_2\", visible = True)\n",
        "        }\n",
        "\n",
        "def format_prompt_template_listener(system_prompt, user_prompt_1, assistant_response, user_prompt_2, prompt_template):\n",
        "    if not user_prompt_1:\n",
        "        raise gr.Error(\"User prompt is mandatory.\")\n",
        "\n",
        "    if not user_prompt_2 and assistant_response:\n",
        "        raise gr.Error(\"When the assistant message is set, the second user prompt is mandatory.\")\n",
        "\n",
        "    if prompt_template == MULTI_TURN and ((user_prompt_2 and not assistant_response) or (not user_prompt_2 and not assistant_response)):\n",
        "        raise gr.Error(\"When generating a multi turn prompt, the assistant message is mandatory.\")\n",
        "\n",
        "    dialog: Dialog = []\n",
        "    if system_prompt: dialog.append({   \"role\": \"system\", \"content\": system_prompt, })\n",
        "    dialog.append({   \"role\": \"user\", \"content\": user_prompt_1, })\n",
        "    if assistant_response: dialog.append({   \"role\": \"assistant\", \"content\": assistant_response, })\n",
        "    if user_prompt_2: dialog.append({   \"role\": \"user\", \"content\": user_prompt_2, })\n",
        "\n",
        "    return {\n",
        "        prompt_output: ChatFormat.encode_dialog_prompt(dialog),\n",
        "        python_output: ChatFormat.format_python_prompt_output(system_prompt, user_prompt_1, assistant_response, user_prompt_2),\n",
        "    }\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1, min_width=200):\n",
        "            gr.Markdown(\"## Configurations\")\n",
        "            prompt_template = gr.Dropdown([SINGLE_TURN, MULTI_TURN], label=\"Prompt Template\", filterable=False, value=SINGLE_TURN)\n",
        "\n",
        "            gr.Markdown(\"## Input Prompts\")\n",
        "            system_prompt = gr.Textbox(label=\"System prompt\", lines=2, placeholder=\"Optional System prompt for the model\")\n",
        "            user_prompt_1 = gr.Textbox(elem_id=\"user_prompt_1\", label=\"User prompt *\", lines=2, placeholder=\"Mandatory user prompt\")\n",
        "            assistant_response = gr.Textbox(label=\"Assistant response\", lines=2, visible=False, elem_id=\"assistant_response\", placeholder=\"Mandatory assistant if prompt template is Multi Turn\")\n",
        "            user_prompt_2 = gr.Textbox(label=\"User prompt *\", lines=2, visible=False, elem_id=\"user_prompt_2\", placeholder=\"Mandatory user prompt if prompt template is Multi Turn\")\n",
        "\n",
        "            prompt_template.input(prompt_template_dropdown_listener, prompt_template, [assistant_response, user_prompt_2])\n",
        "\n",
        "            submit = gr.Button(\"Submit\")\n",
        "\n",
        "        with gr.Column(scale=3, min_width=600):\n",
        "            gr.Markdown(\"## Output\")\n",
        "            with gr.Tab(\"Preview\"):\n",
        "                prompt_output = gr.Code(show_label=True, interactive=False, min_width=600, lines=30)\n",
        "\n",
        "            with gr.Tab(\"Code\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Tab(\"Python\"):\n",
        "                        python_output = gr.Code(label=\"Python Code\", interactive=False, min_width=600, lines=30, language=\"python\")\n",
        "                    \n",
        "\n",
        "    submit.click(format_prompt_template_listener, [system_prompt, user_prompt_1, assistant_response, user_prompt_2, prompt_template], [prompt_output, python_output])\n",
        "\n",
        "demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "prompt_generator",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
